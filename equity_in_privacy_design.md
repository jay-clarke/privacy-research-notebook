Privacy as a Collective Imperative: A Justice-Oriented Framework for Technology Design

Introduction: From Individual Right to Collective Resistance

For decades, the dominant discourse has framed privacy as an individual right, a matter of personal control over one's data, held in opposition to the interests of society.1 This liberal, individualistic paradigm, however, fails to capture the profound and existential role privacy plays for communities that have been systematically disadvantaged. For historically marginalized groups, privacy is not a consumer preference or a lifestyle choice; it is a fundamental condition for survival, a prerequisite for collective action, and a necessary shield against the weaponization of information. It is better understood as a social value and a collective good—a resource essential for democratic participation, social cohesion, and resistance against entrenched systems of oppression.1

The disproportionate privacy harms faced by these communities are not incidental bugs in the digital ecosystem, but rather features that stem from historical power imbalances now encoded in and amplified by technology. Surveillance, algorithmic bias, and data exploitation are the modern manifestations of centuries-old patterns of discrimination and control. This report posits that privacy professionals, as the architects and stewards of our digital world, have an ethical obligation to move beyond reactive, compliance-driven, and universalist frameworks. They must adopt a proactive, justice-oriented approach to design that recognizes the differential risks faced by marginalized populations and works actively to dismantle the digital architecture of harm.

This report will first establish the historical and theoretical foundation for this argument, tracing the legacy of informational oppression into the present day. It will then provide a detailed taxonomy of modern digital harms, demonstrating their interconnected and systemic nature. Following a critique of the dominant privacy paradigms that have proven inadequate to this challenge, the report will introduce a new foundation for privacy grounded in social justice theories. Finally, it will translate this theory into a practical and actionable framework for privacy professionals, offering concrete design principles, methodologies, and tools to build technologies that do not merely avoid harm, but actively sustain, heal, and empower the communities they serve.

Section 1: The Legacy of Oppression and the Weaponization of Information

To understand why privacy is a unique imperative for certain communities, one must first understand the nature of marginalization itself. Historically marginalized groups are communities that have been systematically excluded, oppressed, or disadvantaged due to factors such as race, ethnicity, gender, socioeconomic status, religion, disability, or sexual orientation.3 This marginalization is not a static identity but a dynamic

process rooted in systemic inequalities and power imbalances that limit access to critical resources like education, healthcare, economic opportunity, and political representation.5 This systemic disadvantage creates a state of heightened vulnerability, where the misuse of information is not an abstract risk but a direct threat to safety, liberty, and well-being.

Historical Precedents of Surveillance as Control

The deployment of modern surveillance technologies against marginalized communities is not a new phenomenon but a continuation and amplification of long-standing state practices. History provides a clear and disturbing record of how information and surveillance have been used as tools of political suppression and social control, primarily targeting those who challenge the status quo.

A quintessential example is the FBI's Counterintelligence Program, or COINTELPRO. During the 1950s and 1960s, the U.S. government used the full weight of its surveillance apparatus to monitor, infiltrate, discredit, and disrupt domestic political organizations.7 The targets were not foreign adversaries but American citizens, overwhelmingly from marginalized communities. The FBI extensively surveilled civil rights leaders like Dr. Martin Luther King Jr., as well as Black liberation groups like the Black Panthers, and sought to suppress Black activism across the country.2 The surveillance and violent repression of these movements demonstrate a clear historical link between the denial of privacy and the state's effort to crush political dissent from marginalized populations.

This practice of discriminatory profiling and surveillance has persisted and evolved. The ACLU has documented how, for decades, federal agents have abused their authority by directing surveillance, investigations, and prosecutions to target racial and religious minorities, immigrants, and social justice activists.8 The FBI has engaged in "racial and ethnic mapping," collecting demographic information to build profiles of American communities based on crude stereotypes about which groups commit certain types of crimes.9 Similarly, law enforcement has historically targeted LGBTQ+ communities, raiding social venues and using entrapment tactics to harass and suppress them.2 These historical precedents establish that for marginalized communities, the demand for privacy is not born from a desire for simple seclusion, but from the lived experience of being watched, targeted, and harmed by powerful institutions. The fear of surveillance has a tangible "chilling effect" on free expression, association, and individual autonomy—a harm felt most acutely by those who dare to organize for their rights.2

Connecting Past to Present

The technological evolution from the analog era of the Civil Rights Movement to today's digital panopticon represents a radical amplification, not merely a continuation, of this historical oppression. The surveillance state of the 1960s was technologically limited, dependent on human informants, physical stakeouts, and analog wiretaps.2 Today's law enforcement agencies have access to an arsenal of powerful surveillance tools that dramatically expand their capabilities, including cell-site simulators (Stingrays), automated license plate readers, geofence warrants, predictive policing algorithms, and mass social media monitoring.2 The shift from targeted, labor-intensive surveillance to automated, mass surveillance exponentially increases the state's power to monitor and suppress dissent. This suggests that the imperative for privacy has grown in direct proportion to the capability of surveillance technology. If Dr. King could be so thoroughly surveilled with analog tools, a modern activist faces a far more pervasive and inescapable digital apparatus that can preemptively identify and disrupt organizing before it gains momentum. Thus, privacy is no longer just about protecting against the same harms as in the past, but against a qualitatively different and more potent form of systemic control.

Furthermore, the very condition of being marginalized creates a unique vulnerability to privacy harms. Marginalization is defined by systemic disadvantages, including limited access to legal, financial, and political resources.5 Contesting privacy violations often requires sophisticated legal challenges, technical expertise, and significant financial resources—the very things that marginalized communities are systematically denied.8 This creates a vicious cycle: a lack of power leads to a diminished capacity to defend one's privacy, and this loss of privacy further disempowers the community, making it easier to target and control. Privacy, therefore, is not an isolated concern but is inextricably linked to the broader struggle for power, resources, and self-determination.

Section 2: The Architecture of Digital Harm: A Modern Taxonomy

The historical patterns of oppression are now replicated, automated, and scaled through a complex architecture of digital systems. The harms are not disparate or random; they are interconnected vectors that reinforce one another, creating a comprehensive system of digital control and exclusion. This section provides a taxonomy of these primary vectors of harm.

2.1 The Panoptic Gaze: Discriminatory Surveillance and Social Control

The most direct continuation of historical oppression is the use of modern technology for discriminatory surveillance, which threatens the physical liberty and constitutional rights of marginalized communities.

Policing and Law Enforcement: Government agencies now possess vast, unprecedented powers to peer into people's private lives.8 Authorities like the National Security Agency (NSA) and the FBI exploit legal frameworks such as Section 702 of the Foreign Intelligence Surveillance Act (FISA) and Executive Order 12333 to conduct dragnet surveillance of Americans' data, often without a warrant.8 This surveillance is not applied equally. During the nationwide protests against police brutality following the murder of George Floyd, federal and local law enforcement deployed a deeply invasive, coordinated aerial surveillance campaign to monitor Black Lives Matter protesters.11 Helicopters, airplanes, and drones were used to systematically monitor peaceful protests, with the collected footage channeled into a digital network ominously named "the Big Pipe," accessible to law enforcement agencies for future investigations.11 This mirrors the surveillance of Civil Rights activists decades earlier, but with far more powerful technology. Compounding this is the proliferation of facial recognition technology, which studies have repeatedly shown misidentifies people of color, particularly Black women, at significantly higher rates than their white counterparts.12 This has led to the wrongful arrests of innocent Black men, turning a flawed technology into a direct pipeline to false incarceration.13

Immigration Enforcement: Data-driven technologies are central to modern immigration enforcement. Authorities use sensitive data, such as cellphone location information purchased from data brokers, to track and identify immigrant communities.12 This creates a digital dragnet that can expose an individual's most private movements—where they live, work, worship, and seek healthcare.12 Case studies describe the profound psychological toll this takes, such as the story of "Maria," a mother from El Salvador who was forced to wear a GPS ankle bracelet and was monitored constantly, causing her significant anxiety and stress.14 This pervasive surveillance, which includes monitoring of mixed-status families, creates a constant climate of fear and anxiety, deterring individuals from accessing essential services or participating in community life.14

The "Chilling Effect": Beyond direct enforcement actions, the mere knowledge of being under constant watch has a profound "chilling effect".10 People may alter their behaviors, self-censor their speech, or refrain from associating with activist groups for fear of being targeted.2 This erosion of individual autonomy and free expression is a harm in itself, and it is not borne equally. For communities that must organize and protest to secure their basic rights, this chilling effect is a powerful tool of suppression that undermines the very foundations of democratic action.12

2.2 Algorithmic Stigmatization: Automating Bias and Inequality

If surveillance is the eye of the system, algorithms are its brain, making automated judgments that allocate opportunity and punishment. These systems, often touted as objective and neutral, are increasingly found to be powerful engines for perpetuating and amplifying historical biases.

Predictive Policing: Predictive policing systems use historical crime data to forecast where crime is likely to occur, directing police resources to those areas.17 However, as civil rights organizations like the NAACP and ACLU have argued, this is a deeply flawed premise. Historical crime data is not a neutral reflection of criminal activity; it is a reflection of historical policing practices, which have long over-policed and targeted Black and Brown communities.17 By training on this biased data, predictive policing algorithms create a discriminatory feedback loop: the algorithm predicts crime in a minority neighborhood, police are dispatched there in greater numbers, more arrests are made for minor offenses, and this new arrest data is fed back into the system, "proving" the algorithm was right.21 The result is not the prediction of crime, but the prediction and automation of biased policing.22 Case studies from Chicago's "Strategic Subjects List" and Los Angeles's PredPol program have shown these systems disproportionately flag Black and Latino individuals as high-risk, leading to increased surveillance and arrests without reducing actual crime rates.17

Hiring and Employment: The use of AI in hiring and recruitment is a rapidly growing field, but one fraught with risk. AI tools are used to screen resumes, analyze video interviews, and predict candidate success. However, if these models are trained on a company's historical hiring data, which may reflect decades of conscious or unconscious bias, the AI will learn to replicate those patterns.23 For example, an AI trained on the profiles of a company's current, predominantly male engineering team may learn to penalize resumes that include words associated with women's colleges or clubs.24 This can effectively prevent capable candidates from marginalized groups from even getting an interview. The landmark 2024 class-action lawsuit,

Mobley v. Workday, alleges that Workday's widely used AI-driven recruitment software engages in a "pattern and practice" of discrimination based on race, age, and disability, systematically disqualifying protected candidates.23

Credit, Housing, and Healthcare: Algorithmic bias extends to nearly every critical life opportunity. In finance, algorithms used for credit scoring can perpetuate historical economic disparities, denying loans to qualified applicants in marginalized communities. In housing, algorithms have been used to discriminate in ad delivery, preventing people of color from seeing housing opportunities.7 In healthcare, AI systems designed to predict patient needs have been found to exhibit racial bias, leading to Black patients receiving a lower standard of care because the algorithm used healthcare cost as a proxy for health needs, failing to account for the fact that less money is spent on Black patients for the same level of need.13

2.3 Digital Redlining: Replicating Segregation in Cyberspace

The historical practice of redlining—where banks drew red lines on maps around minority neighborhoods to deny them loans and investment—has found a new and insidious life in the digital world. Digital redlining creates and perpetuates inequities through digital technologies, content, and infrastructure, effectively creating digital ghettos.

Infrastructure and Access: The most fundamental form of digital redlining is the discriminatory deployment of internet infrastructure. Investigations have revealed that major internet service providers (ISPs) like AT&T have systematically under-invested in upgrading their networks to high-speed fiber in low-income and minority neighborhoods, while deploying state-of-the-art services in wealthier, whiter parts of the same cities.27 Residents in these digitally redlined areas are often left with slow, unreliable legacy DSL networks while paying comparable prices to those in affluent areas with fiber.28 This digital divide is not a matter of convenience; it is a profound barrier to opportunity, limiting residents' ability to apply for jobs, access telehealth services, attend online classes, or participate in the modern economy.27

Service and Content Exclusion: Digital redlining also manifests in the services and content available to different communities. For example, Amazon was found to have excluded many predominantly Black neighborhoods from its Prime same-day delivery service.27 Geographically-based games like Pokémon Go have been shown to offer more features and rewards in less diverse areas.27 These decisions, often driven by calculations of profitability, reinforce patterns of disinvestment and exclusion.

Algorithmic Exclusion: Perhaps the most subtle form of digital redlining occurs through algorithms. In 2019, the U.S. Department of Housing and Urban Development (HUD) charged Facebook with violating the Fair Housing Act for allowing advertisers to use its platform to exclude specific groups from seeing ads for housing and employment.7 Advertisers could draw a virtual "red line" around a geographic area or exclude users based on their "ethnic affinity," gender, or other protected characteristics, effectively making opportunities invisible to entire communities.27

2.4 Predatory Data Exploitation: Targeting Vulnerability for Profit

The final vector of harm involves the use of personal data to actively target vulnerable populations with exploitative and extractive products and services. This practice of predatory advertising manipulates individuals by leveraging their known financial, cognitive, or emotional vulnerabilities.31

Case Study: For-Profit Colleges: A stark example is the for-profit education industry. The Student Borrower Protection Center's "Mapping Exploitation" report reveals that for-profit colleges are far more likely to be geographically located in and around Black and Latino neighborhoods.32 These institutions use sophisticated marketing to target low-income consumers, veterans, and communities of color with the "allure of opportunity," promising social mobility and good jobs.32 In reality, they often sell expensive, low-quality degrees that provide little return on investment, leaving students—particularly Black and Latino students—saddled with crippling debt and further from economic security. The report describes this as a "wealth-stripping" business model that preys on systemic inequities.32

Other Predatory Practices: This predatory model is not unique to education. Data brokers create and sell lists of vulnerable consumers, such as "gullible" individuals or those suffering from dementia, to be targeted by marketers.27 Financially unstable individuals are targeted with ads for high-interest payday loans.31 Women and minorities are disproportionately targeted by deceptive Multi-Level Marketing (MLM) schemes that promise financial independence but result in financial loss for the vast majority of participants.33 Health-harming industries, like those selling junk food and alcohol, are known to aggressively market to low-income and minority children.34

These four vectors of harm—surveillance, algorithmic bias, redlining, and predation—are not discrete phenomena. They are deeply interconnected, forming a cohesive architecture of digital oppression. A community that has been digitally redlined, for instance, suffers from limited economic and educational opportunities. This economic precarity makes its residents prime targets for predatory advertising for for-profit colleges or high-interest loans. Simultaneously, the historical data reflecting the poverty and crime rates (which are often a proxy for poverty and the results of over-policing) in these same neighborhoods are fed into predictive policing algorithms. This, in turn, leads to increased surveillance and a greater police presence, which further stigmatizes the community and generates more biased data to feed back into the system. The cycle is self-perpetuating and comprehensive. A privacy professional cannot, therefore, address one of these harms in isolation. Fixing a biased hiring algorithm is a hollow victory if the targeted community never gets the broadband access needed to find and apply for the job in the first place. This demonstrates the absolute necessity of a systemic, not siloed, approach to privacy design. A critical function for the privacy professional, then, is not just to build systems but to deconstruct the pervasive and misleading narratives of neutrality that surround them. The work is as much about communication, advocacy, and social analysis as it is about code.

Section 3: The Failure of Universalist and Compliance-Driven Frameworks

The existing paradigms for privacy protection have proven profoundly inadequate for addressing the systemic and disproportionate harms faced by marginalized communities. The dominant models, rooted in individual control and legal compliance, fail to account for vast power asymmetries and often serve to legitimize the very systems that cause harm.

3.1 The "Notice and Consent" Fallacy

The cornerstone of modern data protection law in many jurisdictions is a set of "fair information practice principles" (FIPPs) that emphasize individual control through mechanisms like notice and consent.35 In theory, this approach empowers individuals by informing them of data practices and allowing them to make choices. In practice, this model has failed.35

The critique of FIPPs is multifaceted. At their inception, they were broad, aspirational principles that included substantive goals like data quality and use limitation. However, as they were translated into law, they were reduced to a narrow, procedural, and legalistic framework focused on notice, choice, access, and security.35 The result is an onslaught of privacy notices that are effectively meaningless. They are often ignored, written in opaque legalese, and so long that it would take an average user an estimated 244 hours per year to read the policies of every website they visit.35 Even when a choice is presented, it is rarely meaningful. It is often a "take-it-or-leave-it" proposition, forcing users to agree to invasive terms to access essential services.35

This entire framework is built upon the myth of a rational, empowered "average user" who has the time, expertise, and cognitive capacity to meticulously manage their privacy preferences for every service they use.35 This model completely ignores the realities of "consent fatigue," where users, overwhelmed by constant requests, simply click "agree" without comprehension.35 This model is particularly inequitable. It places the burden of protection on the individual, disadvantaging those with lower digital literacy, less time, or who are in positions of dependency where they cannot realistically refuse consent to a potential employer, landlord, or government service provider.

This "notice and consent" model does not just fail to protect users; it actively harms marginalized communities by creating an illusion of control while simultaneously legitimizing extractive data practices. When a person from a marginalized community, who may be dependent on a digital service for employment or access to benefits, clicks "agree," they are not making a free choice. They are participating in a coercive transaction. In doing so, they are forced to provide legal cover for a system that may then use their data to discriminate against them, whether through a biased hiring algorithm or predatory advertising. The privacy framework itself thus becomes a tool of oppression, shifting responsibility from the powerful data controller to the vulnerable data subject under the false pretense of individual empowerment and choice. This is a profound contradiction that privacy professionals must recognize and dismantle.

3.2 The Limits of Compliance

In recent years, comprehensive data protection regulations like the European Union's General Data Protection Regulation (GDPR) and the California Consumer Privacy Act (CCPA) have emerged. While these laws represent important steps forward, a purely compliance-driven approach to privacy is also insufficient to achieve equity.37

Organizations often treat these regulations as a ceiling for their obligations, not a floor. The primary motivation becomes avoiding hefty fines and legal battles, rather than a genuine commitment to preventing substantive harm to individuals and communities.37 This can lead to a "transactional approach" to privacy, where legal and technical teams work in silos to ensure that specific, narrow requirements are met for different jurisdictions, losing sight of the holistic, systemic harms a product might cause.39 A system can be fully compliant with GDPR's data processing principles yet still produce wildly discriminatory outcomes if its underlying algorithm is trained on biased data. The law may require a Data Protection Impact Assessment (DPIA), but if that assessment only focuses on compliance risks and not on potential disparate impacts on marginalized groups, it fails to address the core issue of justice. This reveals that a compliance-focused mindset can inadvertently entrench inequity by optimizing for legal defensibility rather than for human dignity and social justice.

3.3 The Critique of Universalism

Both the "notice and consent" model and many compliance frameworks are rooted in a universalist assumption: that a single set of rules or principles can be applied equally to all people in all contexts. This "one-size-fits-all" approach is fundamentally flawed because it ignores the reality that privacy risks and harms are not distributed equally.

The framework of Targeted Universalism offers a powerful critique of this approach. It argues for setting universal goals (e.g., "all members of society should have equitable access to housing opportunities") but using targeted strategies to achieve that goal, based on an analysis of how different groups are situated in relation to structures of power and geography.40 A universal privacy policy that gives every user the same set of controls is a form of false equality; it treats a privileged user who faces minimal risk and a marginalized user who faces existential threats as if their needs are identical. A targeted universalist approach, in contrast, would recognize the differential risk and might require much stronger, non-negotiable protections for data processing that affects vulnerable communities.

Drawing an analogy from Post-Development Theory, which critiques the imposition of Western, universalist models of "development" on diverse global cultures, one can see how universalist privacy models can be a form of "ethnocentric" design.41 They are often designed by and for a dominant culture, reflecting its values and threat models, and can fail to respect the unique contexts, needs, and vulnerabilities of other communities. For privacy to be meaningful, it must be contextual and sensitive to the specific power dynamics and historical injustices that shape a community's experience.

Section 4: A New Foundation: Privacy Through a Social Justice Lens

To move beyond the failed paradigms of universalism and compliance, privacy professionals require a new theoretical foundation—one that is equipped to analyze power, context, and systemic harm. Three interconnected fields of thought provide the necessary concepts: Contextual Integrity, Critical Race Theory, and Feminist Data Studies. Together, they form a powerful, multi-layered analytical framework for designing equitable privacy.

4.1 Contextual Integrity (CI): Privacy as Appropriate Flow

Developed by philosopher Helen Nissenbaum, the theory of Contextual Integrity (CI) offers a profound shift in how to conceptualize privacy. CI defines privacy not as secrecy or individual control, but as the preservation of appropriate information flows that are consistent with the norms of a given social context.43 There is no blanket prohibition on information sharing; rather, the theory provides a framework for evaluating whether a particular flow of information is legitimate and morally justifiable.44

CI analyzes information flows using five key parameters:

Data Subject: The individual the information is about.

Sender: The party transmitting the information.

Recipient: The party receiving the information.

Information Type: The nature of the data being shared (e.g., health data, financial data, location data).

Transmission Principle: The constraints under which the information flows (e.g., with consent, under duress, by law, for a price).43

A privacy violation occurs when a new technology or practice disrupts the established norms of a context by altering one or more of these parameters in an inappropriate way.45 For example, sharing your health concerns (information type) with your doctor (recipient) is a normal and appropriate flow within the healthcare context. However, if your doctor then sells that same information to a pharmaceutical company (a new recipient) for marketing purposes (a new transmission principle), the contextual integrity is breached, and a privacy violation has occurred.46

This framework provides a precise vocabulary for diagnosing the digital harms detailed in Section 2. Predictive policing, for instance, violates CI by taking information about individuals gathered in one context (e.g., public records of minor infractions) and transmitting it to a new recipient (a predictive algorithm) under a new principle (statistical risk assessment) to make high-stakes decisions (where to deploy police patrols) that violate the norms of due process and equal protection.47 CI moves the analysis from a vague concern about "data collection" to a specific diagnosis of an inappropriate information flow.

4.2 Critical Race Theory (CRT) and Privacy

While Contextual Integrity provides the "how" of diagnosing privacy violations, Critical Race Theory (CRT) provides the "why." CRT is an academic framework that examines how race and racism have shaped legal systems and societal structures.48 Core tenets of CRT include the understanding that racism is systemic and institutional, not merely a matter of individual prejudice; that race is a social construct used to maintain power hierarchies; and that supposedly "neutral" or "color-blind" laws and technologies can function to perpetuate racial injustice.48

Applying a CRT lens to privacy reveals that the disproportionate surveillance and data exploitation of communities of color is not an accident or an anomaly. It is a feature of a socio-technical system designed to maintain existing racial hierarchies.7 When a facial recognition system exhibits higher error rates for Black faces, CRT pushes us to see this not just as a technical problem of a flawed dataset, but as the digital manifestation of a society that has historically rendered Blackness both hypervisible to the police and invisible to the engineers of technology.13 Surveillance of Black communities, from COINTELPRO to the monitoring of Black Lives Matter activists, is understood as a continuous practice of social control.7 CRT insists that we cannot understand privacy harms without understanding their relationship to the long history of racial oppression.

4.3 Feminist Data Studies and Surveillance

Feminist scholarship makes critical contributions by expanding the definition of surveillance and centering the experiences of those marginalized by patriarchal structures. A feminist approach to surveillance looks beyond the traditional state-on-citizen model to include private and interpersonal forms of monitoring, such as the objectifying "male gaze," and shows how this is replicated and amplified online.50 It challenges the default assumption in surveillance studies of a uniform, male "surveilled subject," demonstrating that technology is experienced differently along gendered and intersectional lines.50

The emerging field of Data Feminism, articulated by Catherine D'Ignazio and Lauren F. Klein, offers a set of principles for a more ethical and empowering data science. These principles include challenging power hierarchies, embracing pluralism, examining how context shapes data, and valuing multiple forms of knowledge, including the lived experiences of marginalized communities.51 This framework provides a direct bridge from the critique of power to the practice of building more just data systems.

These three theoretical frameworks are not competing but are powerfully complementary, forming a multi-layered analytical lens for the privacy professional. Critical Race Theory and Feminist Data Studies provide the essential political and social analysis, identifying the structures of power and historical context that explain why certain groups are targeted and experience harm differently. Contextual Integrity then provides the precise analytical and descriptive vocabulary to diagnose how these systemic harms manifest as specific, inappropriate information flows within social contexts.

For example, a privacy professional examining the deployment of a facial recognition system in public housing can use CRT to understand why this is a profound racial justice issue, tied to histories of segregation and over-policing. They can then use CI to perform a granular analysis of the specific privacy violation: data subjects (tenants, who are disproportionately people of color) have their biometric data (information type) captured by a sender (the landlord's camera system) and potentially sent to a recipient (a police database) under a principle of coercion (as a non-negotiable condition of housing). This flow violates the established norms of the home as a private sanctuary. This synthesis elevates the professional's work from a vague awareness of "bias" to a rigorous, precise diagnosis of a privacy violation grounded in a robust theory of justice.

Section 5: Design Justice in Practice: A Guide for Privacy Professionals

Armed with a new theoretical foundation, privacy professionals must translate these critical perspectives into action. This requires a radical departure from traditional, top-down design processes and an embrace of methodologies that actively center the communities most impacted by technology. The fields of Design Justice, Anti-Oppressive Design, and Co-Design offer a roadmap for this transformation.

5.1 Adopting Design Justice Principles

The Design Justice Network, an international community of practitioners, has articulated a set of principles that aim to fundamentally rethink the design process.52 This framework moves beyond the often-exclusionary model of "human-centered design" toward community-led and controlled outcomes. For privacy professionals, adopting these principles means shifting their entire orientation to their work.

The ten principles provide a clear guide.53 Several are particularly crucial for reimagining privacy design:

Principle 2: We center the voices of those who are directly impacted by the outcomes of the design process. This is the foundational principle. It requires that privacy design not be an abstract exercise conducted by experts in an office, but a collaborative process deeply engaged with the communities who will bear the risks of failure.

Principle 3: We prioritize design's impact on the community over the intentions of the designer. Good intentions are not enough. A privacy system designed with the intention of protecting users can still have a harmful impact if it fails to account for the specific threats faced by a marginalized group. This principle demands rigorous assessment of real-world consequences.

Principle 5: We see the role of the designer as a facilitator rather than an expert. This principle challenges the traditional hierarchy of the design process. The privacy professional's role is not to dictate solutions from on high, but to facilitate a process where the community's own expertise can emerge and shape the outcome.

Principle 6: We believe that everyone is an expert based on their own lived experience. This directly validates the knowledge of marginalized individuals. A person who has experienced discriminatory policing is an expert on the harms of surveillance in a way no technologist can be. Their lived experience is critical data for the design process.

Principle 8: We work towards sustainable, community-led and -controlled outcomes. The ultimate goal is not just to build a product, but to build community power and self-determination.

5.2 Implementing Anti-Oppressive Design

Anti-oppressive design provides a set of practical methods for putting the principles of Design Justice into action. It is a framework that challenges the systems and structures that perpetuate inequality and marginalization.56 It requires designers to actively acknowledge how power operates in everyday situations and to take deliberate action to cede their own power to those who have less of it.57

A useful framework for this practice involves several stages of engagement 58:

Sense-Making: Develop an orientation toward power asymmetries. Before any solution is proposed, the design process must begin by naming and centering the existing structural inequities that define the problem space. This involves identifying the institutional logics and power dynamics at play.58 For a privacy professional, this means starting a project not with a technical specification, but with a power analysis.

Meaning-Making: Take responsibility for how position determines power. Designers must interrogate their own methodologies and tools, which are never neutral.58 Traditional design thinking rubrics can possess an "illusion of neutrality" that masks the designer's own interests and biases.58 This stage requires making the positionality of the design team—their identities, privileges, and worldviews—transparent and accountable.

Practice Reciprocity and Care. Anti-oppressive design moves away from transactional research relationships ("extracting" data from users) toward building relationships rooted in care and reciprocity.57 This means compensating community members for their expertise, giving back to the community in meaningful ways, and focusing on team care to handle the emotional weight of engaging with difficult issues.57

Embrace Humility and Openness. An anti-oppressive practitioner must be ready to get it wrong, to be called out on gaps in their knowledge, and to apologize and learn.57 This vulnerability is key to building the trust necessary for genuine collaboration.

5.3 The Power of Co-Design

Co-design is the primary methodology through which the principles of Design Justice and Anti-Oppressive Design are realized. It is a collaborative approach that brings together people with different forms of expertise—including technical expertise, cultural knowledge, and, crucially, lived experience—to design solutions together.59 The core principles of co-design are sharing power, prioritizing relationships, using participatory means, and building capability for everyone involved.59

Successfully co-designing privacy solutions with marginalized communities requires careful attention to process. Case studies on co-designing with groups at the "Base of the Pyramid" (BOP) in developing countries or with BIPOC families in the U.S. highlight key challenges and strategies.61 A common barrier is an "incorrect focus," where projects are driven by the goals of a technology company or donor rather than the pressing needs of the community, leading to poor engagement.61 Effective co-design requires that the project's goals are aligned with the community's self-identified needs from the very beginning. The process must be flexible and use a variety of participatory tools—such as visual aids, storytelling, and prototyping—to allow people to express themselves in multiple ways, moving beyond text-heavy reports and presentations.59

Adopting these justice-oriented design practices leads to a fundamental redefinition of the "deliverable" for a privacy professional. In a traditional model, the output is a product, a feature, or a compliance report. Success is measured by metrics like user adoption or the absence of legal fines. In a Design Justice framework, the process itself becomes a primary outcome. The act of building trust, sharing power, facilitating dialogue, and enhancing a community's capacity to advocate for itself is a core objective. The technology that emerges from this process is secondary to the social transformation and empowerment that the process enables. For a privacy professional, this means success is not measured by a passed Privacy Impact Assessment, but by a co-design process where community members feel heard, valued, and empowered, and where the final outcome is one they truly own and control. This requires a radical shift in how privacy projects are scoped, budgeted, timed, and evaluated.

Section 6: The Privacy Professional's Toolkit for Equitable Design

To operationalize a justice-oriented approach, privacy professionals need a new set of tools that are designed to surface and address issues of equity. Standard tools like Privacy Impact Assessments are necessary but insufficient. The modern privacy toolkit must be expanded to include methodologies like Equity Impact Assessments and a re-framing of technical practices like data minimization as core equity strategies.

6.1 Equity Impact Assessments (EIAs)

An Equity Impact Assessment (EIA) is a systematic process used to proactively examine how a proposed policy, program, or technology could affect different population groups and to anticipate whether it is likely to widen or narrow existing equity disparities.64 Unlike a standard PIA, which focuses on data protection compliance, an EIA centers the analysis on potential discriminatory impacts and social justice outcomes.66

A practical guide for conducting an EIA in a technology context involves several key steps, synthesized from various public health and education models 67:

Screening and Scoping: The process begins by defining the project and, crucially, identifying the populations that could be affected, with a specific focus on historically marginalized groups. This requires research into the demographics and socio-economic conditions of potential user communities.68

Community Engagement and Data Gathering: This is the most critical phase. The EIA team must move beyond assumptions and engage authentically with the communities identified in the scoping phase. This involves participatory methods like focus groups, co-design workshops, and interviews with community leaders and individuals with lived experience to understand potential impacts from their perspective.59 The goal is to answer the question: "How might this technology be experienced by this specific community, given their history and current context?"

Analysis of Potential Impacts: The team analyzes the gathered information to answer a set of critical questions: How might this initiative unintentionally worsen existing inequities or create new ones? Does it rely on data known to contain historical biases? Does it create barriers to access for people with disabilities, lower incomes, or different levels of digital literacy?.64

Mitigation and Alternatives: Based on the analysis, the team develops concrete strategies to mitigate negative impacts and maximize positive ones. This could involve substantive changes to the technology's design, the data it uses, or the policies governing its use. Importantly, the EIA should also consider the alternative of not deploying the technology if the potential for harm is too great and cannot be adequately mitigated.

Monitoring and Evaluation: An EIA is not a one-time event. The plan must include a process for monitoring the technology's real-world impact over time to see if it is, in fact, narrowing or widening equity gaps, and to make adjustments as needed.64

The following table illustrates the fundamental differences between a standard, compliance-focused Privacy Impact Assessment and a justice-oriented Equity Impact Assessment. This comparison provides a clear visualization of the paradigm shift this report advocates, moving the conversation from "Are we compliant?" to "Are we just?".

6.2 Data Minimization as an Equity Strategy

Data minimization—the principle of collecting, using, and retaining only the data that is strictly necessary for a specific, defined purpose—is a foundational concept in privacy law.69 However, in a justice-oriented framework, it must be understood not just as a privacy best practice, but as a fundamental tool for promoting equity.

The equity argument for data minimization is straightforward and powerful: less data collected means less data available to be misused. It directly reduces the attack surface for both security breaches and the insidious harm of algorithmic bias.71 If a hiring algorithm is never given data on a candidate's gender or the name of their college, it cannot use that data to discriminate. If a system does not collect precise location data, that data cannot be sold to brokers or subpoenaed by law enforcement to track protesters. Data minimization is a direct method of reducing the power of data-driven systems and limiting their potential for harm.

Practical data minimization techniques that privacy professionals should champion include:

Strict Purpose Limitation and Data Mapping: Before any data is collected, there must be a clearly defined and legitimate purpose. A data mapping exercise should be conducted to identify all data points being collected and to challenge the necessity of each one. Nonessential information should be eliminated at the source.69

Privacy-Enhancing Technologies (PETs): PETs are a class of technologies that enable data analysis and system functionality while minimizing the exposure of raw personal data.

Federated Learning: This technique allows AI models to be trained on decentralized data across multiple devices (e.g., mobile phones) without the raw data ever being sent to a central server. This is a powerful way to build intelligent systems without creating massive, vulnerable repositories of personal information.71

Differential Privacy: This method involves adding mathematically calibrated statistical noise to a dataset before it is analyzed. The noise is just enough to make it impossible to re-identify any single individual's contribution, while still allowing for accurate aggregate analysis. This protects individual privacy while enabling valuable research.71

Anonymization, Tokenization, and Data Masking: These techniques involve replacing sensitive, personally identifiable information (PII) with non-identifiable placeholders. Tokenization replaces data with a non-sensitive token, while data masking replaces it with realistic but fake data that retains the statistical properties needed for testing or analysis.71

The choice of which tools to include in a privacy professional's toolkit is, in itself, an ethical and political decision that reflects a commitment to justice. Opting to conduct a robust EIA instead of a standard PIA is an explicit choice to prioritize community impact over mere compliance. Deciding to use co-design methods is an explicit choice to cede power from the "expert" designer to the community. Therefore, privacy professionals must become conscious of the values embedded in their own processes and advocate for the adoption of a justice-oriented toolkit, even when it requires organizational change and a fight for the necessary time and resources.

Conclusion: Envisioning and Building Liberatory Privacy Futures

The analysis presented in this report paints a sobering picture of how digital technologies can become instruments of systemic oppression, replicating and amplifying historical injustices against marginalized communities. However, a diagnosis of the problem is not enough. The ultimate task for privacy professionals is not merely to mitigate harm, but to actively participate in the creation of more just and equitable technological futures. This requires a move beyond reactive problem-solving and an embrace of radical imagination.

The practice of speculative design offers a powerful framework for this work. Speculative design uses imagination, narrative, and prototyping not to predict the future, but to explore alternative possibilities and provoke critical thought about the kind of future we want to create.76 It challenges us to ask "What if?" questions that break free from the constraints of current technological and political realities.77 Privacy professionals can use speculative methods to envision futures where technology is designed not just to protect, but to empower. What if a data trust was governed by and for a specific marginalized community, allowing them to use their collective data for advocacy and resource allocation? What if privacy-enhancing technologies were deployed not just to prevent corporate surveillance, but to enable secure, anonymous communication for activists organizing against an oppressive regime? By crafting future scenarios, narratives, and "future prototypes"—artifacts from a possible future—professionals can spark essential conversations about values and inspire the development of truly liberatory technologies.76

Inspiration for this work can be drawn from artistic and cultural movements like Afrofuturism, which blends science fiction, fantasy, and social justice to construct new narratives and envision futures from the perspective of the African diaspora.79 Afrofuturism is a powerful demonstration of how imagination can be a tool of resistance and world-building for communities that have been historically excluded from defining the future. It reminds us that the act of envisioning a different world is the first step toward creating it.

The final call to action is this: the role of the privacy professional in the 21st century must evolve. It is no longer sufficient to be a compliance officer or a risk mitigator. The challenges of our time demand that privacy professionals become proactive advocates for social justice, facilitators of community power, and architects of a more equitable digital world. By embracing the critical theories that expose the workings of power, adopting the justice-oriented design practices that center the most impacted, and wielding the practical tools that can dismantle digital harms, they can help build technologies that sustain, heal, and empower all communities. The future is not something to be predicted; it is something to be designed. The imperative is to ensure it is designed with justice for all.

Works cited

The social value of privacy, the value of privacy to society and human rights discourse (Chapter 12) - Social Dimensions of Privacy - Cambridge University Press, accessed July 6, 2025,

Surveillance is a Racial Justice Issue - ACLU of Wisconsin, accessed July 6, 2025,

oxford-review.com, accessed July 6, 2025,

Historically Marginalised Groups - Definition and Explanation - The Oxford Review, accessed July 6, 2025,

Diversity and Marginalized Communities - ACTEC, accessed July 6, 2025,

Understanding Marginalized Communities - Number Analytics, accessed July 6, 2025,

Examining the intersection of data privacy and civil rights - Brookings Institution, accessed July 6, 2025,

Harris on Surveillance | American Civil Liberties Union, accessed July 6, 2025,

Mapping the FBI | American Civil Liberties Union, accessed July 6, 2025,

Surveillance vs. Privacy — Balancing Security and Freedom in Cybersecurity - Medium, accessed July 6, 2025,

ACLU Seeks Information on Government's Aerial Surveillance of Protesters, accessed July 6, 2025,

Privacy and Surveillance - ACLU Massachusetts, accessed July 6, 2025,

Privacy & Racial Justice – EPIC – Electronic Privacy Information ..., accessed July 6, 2025,

Surveillance and Social Justice - Number Analytics, accessed July 6, 2025,

CBP Releases December 2024 Monthly Update | U.S. Customs and Border Protection, accessed July 6, 2025,

About 1 in 4 U.S. adults worry they or someone close to them could be deported, accessed July 6, 2025,

Racial Justice and Predictive Policing - Number Analytics, accessed July 6, 2025,

Algorithmic Justice or Bias: Legal Implications of Predictive Policing Algorithms in Criminal Justice - The Johns Hopkins Undergraduate Law Review, accessed July 6, 2025,

The Use of Artificial Intelligence in Predictive Policing - NAACP, accessed July 6, 2025,

Artificial Intelligence in Predictive Policing Issue Brief - NAACP, accessed July 6, 2025,

Does Predictive Policing Lead to Biased Arrests? Results From a Randomized Controlled Trial, accessed July 6, 2025,

Report Claims Predictive Policing Targets Marginalized - Davis Vanguard, accessed July 6, 2025,

Algorithmic Bias in AI Employment Decisions - Journal of ..., accessed July 6, 2025,

ACM Conference on Fairness, Accountability, and Transparency - Wikipedia, accessed July 6, 2025,

www.hklaw.com, accessed July 6, 2025,

Artificial Intelligence in Hiring: Diverging Federal, State Perspectives on AI in Employment?, accessed July 6, 2025,

Digital redlining - Wikipedia, accessed July 6, 2025,

Digital Redlining: What It Is and How It Continues to Impact Communities - CNET, accessed July 6, 2025,

Digital Redlining and the Black Rural South - Capital B News, accessed July 6, 2025,

Digital Redlining | County Health Rankings & Roadmaps, accessed July 6, 2025,

Predatory advertising - Wikipedia, accessed July 6, 2025,

MAPPING EXPLOITATION - Student Borrower Protection Center, accessed July 6, 2025,

Multilevel Marketing, an Unwinnable Lottery: How MLMs Illegally Target Women and Minorities Using Deceptive and Predatory Recrui - Georgetown Law, accessed July 6, 2025,

Policy solutions to predatory marketing: Protecting today's children and youth from a lifetime of chronic disease | NCD Alliance, accessed July 6, 2025,

The Failure of Fair Information Practice Principles - ResearchGate, accessed July 6, 2025,

The Cost of Data Privacy Negligence (And How to Avoid It) - Countly, accessed July 6, 2025,

Data privacy compliance challenges: navigating the regulatory landscape - TrustCommunity, accessed July 6, 2025,

A Comparative Analysis of the NIST Privacy Framework vs. the EU's GDPR - Securiti.ai, accessed July 6, 2025,

NIST Privacy Framework - Hyperproof, accessed July 6, 2025,

Targeted Universalism | Othering & Belonging Institute, accessed July 6, 2025,

Postdevelopment theory - Wikipedia, accessed July 6, 2025,

Universalism - IB Global Politics, accessed July 6, 2025,

Contextual integrity - Wikipedia, accessed July 6, 2025,

998: Primer on the Contextual Integrity Theory of Privacy with Philosopher Helen Nissenbaum - Voices of VR Podcast, accessed July 6, 2025,

Contextual Integrity Up and Down the Data Food Chain - Helen Nissenbaum, accessed July 6, 2025,

Going Against the (Appropriate) Flow: A Contextual Integrity Approach to Privacy Policy Analysis - Submitted to FTC PrivacyCon20, accessed July 6, 2025,

"Privacy as Contextual Integrity" by Helen Nissenbaum - UW Law Digital Commons, accessed July 6, 2025,

Critical race theory - Wikipedia, accessed July 6, 2025,

Data & Society — Race, Surveillance, Resistance, accessed July 6, 2025,

Surveillance as a Feminist Issue | Privacy International, accessed July 6, 2025,

Data + Feminism Lab, MIT, accessed July 6, 2025,

Design Justice Network Principles - Digital Government Hub, accessed July 6, 2025,

Design Justice Network Principles — R / D - Reading Design, accessed July 6, 2025,

Design Justice Network - Allied Media Projects, accessed July 6, 2025,

Read the Principles — Design Justice Network, accessed July 6, 2025,

Embracing Anti-Oppressive Practice: Building a More Inclusive and Equitable Society, accessed July 6, 2025,

Opening up inclusion: Anti-oppression and inclusive design ..., accessed July 6, 2025,

A framework for anti-oppressive design | by Jessica Meharry - Medium, accessed July 6, 2025,

What is co-design? - Beyond Sticky Notes, accessed July 6, 2025,

What Is Codesign? | IxDF, accessed July 6, 2025,

Co-design with marginalised people: designers' perceptions of barriers and enablers, accessed July 6, 2025,

Co-design Tensions Between Parents, Children, and Researchers Regarding Mobile Health Technology Design Needs and Decisions: Case Study, accessed July 6, 2025,

(PDF) Generative Tools for Co-designing - ResearchGate, accessed July 6, 2025,

Diversity, Equity, and Inclusion Resource and Advisory Committee (DRAC) Equity Impact Assessment Tool - COMSEP, accessed July 6, 2025,

Health Equity Impact Assessment (HEIA) - CAMH, accessed July 6, 2025,

Equity Review Tool - Every Learner Everywhere, accessed July 6, 2025,

Evidence- and Consensus-Based Digital Healthcare Equity Framework - Johns Hopkins Bloomberg School of Public Health, accessed July 6, 2025,

Impact Assessment 101 - Comprehensive Guide to Business Impact Analysis - Simfoni, accessed July 6, 2025,

Ensuring Data Privacy in AI: Best Practices and Challenges - Rapid Innovation, accessed July 6, 2025,

Data Privacy In AI-Driven Learning And Ethical Considerations - eLearning Industry, accessed July 6, 2025,

AI and Data Privacy: Practical Tips for Securing AI Systems | SmartDev, accessed July 6, 2025,

Privacy & Surveillance Archives - AI Now Institute, accessed July 6, 2025,

Tech Experts Share Tested And Effective Data Minimization Strategies - Forbes, accessed July 6, 2025,

Equitable differential privacy - PMC, accessed July 6, 2025,

Best Practices to Mitigate AI Data Privacy Concerns - Salient Process, accessed July 6, 2025,

Shaping futures with Speculative design - AFRY, accessed July 6, 2025,

How to Get Started with Speculative Design: A Step-by-Step Guide - Onething Design, accessed July 6, 2025,

Speculative Design for long term future foresight and strategy - Trend Atelier, accessed July 6, 2025,

How Afrofuturism mixes science fiction and social justice | PBS News, accessed July 6, 2025,